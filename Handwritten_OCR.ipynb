{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a61f75d5",
   "metadata": {},
   "source": [
    "### Installing neccessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2ca50f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TensorFlow\n",
    "import tensorflow as tf\n",
    "import tensorflow.data as tfd\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Others\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as implt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2d622c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define AUTOTUNE for TensorFlow dataset\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaae4cb",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "275b31d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define necessary variables\n",
    "IMG_WIDTH = 200\n",
    "IMG_HEIGHT = 50\n",
    "IMG_SIZE = (IMG_WIDTH, IMG_HEIGHT)\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 50\n",
    "LEARNING_RATE = 1e-3\n",
    "MODEL_NAME = \"CharacterRecognition-Model\"\n",
    "TRAIN_SIZE = BATCH_SIZE * 1500\n",
    "VALID_SIZE = BATCH_SIZE * 1000\n",
    "TEST_SIZE  = BATCH_SIZE * 600\n",
    "\n",
    "# Training callbacks\n",
    "CALLBACKS = [\n",
    "    EarlyStopping(patience=10, restore_best_weights=True),\n",
    "    ModelCheckpoint(filepath=MODEL_NAME + \".h5.keras\", save_best_only=True)\n",
    "]\n",
    "\n",
    "# Paths\n",
    "train_csv_path = r'D:\\Projects\\Handwritten OCR\\Dataset\\CSV\\written_name_train.csv'\n",
    "valid_csv_path = r'D:\\Projects\\Handwritten OCR\\Dataset\\CSV\\written_name_validation.csv'\n",
    "test_csv_path = r'D:\\Projects\\Handwritten OCR\\Dataset\\CSV\\written_name_test.csv'\n",
    "train_image_dir = r'D:\\Projects\\Handwritten OCR\\Dataset\\train_v2\\train'\n",
    "valid_image_dir = r'D:\\Projects\\Handwritten OCR\\Dataset\\validation_v2\\validation'\n",
    "test_image_dir = r'D:\\Projects\\Handwritten OCR\\Dataset\\test_v2\\test'\n",
    "\n",
    "# Setup random seeds for numpy and TensorFlow\n",
    "np.random.seed(2569)\n",
    "tf.random.set_seed(2569)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "87453fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the csv flies\n",
    "train_csv = pd.read_csv(train_csv_path)[:TRAIN_SIZE]\n",
    "test_csv = pd.read_csv(test_csv_path)[:TEST_SIZE]\n",
    "valid_csv = pd.read_csv(valid_csv_path)[:VALID_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d8ae7bc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>IDENTITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TRAIN_00001.jpg</td>\n",
       "      <td>BALTHAZAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TRAIN_00002.jpg</td>\n",
       "      <td>SIMON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TRAIN_00003.jpg</td>\n",
       "      <td>BENES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TRAIN_00004.jpg</td>\n",
       "      <td>LA LOVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TRAIN_00005.jpg</td>\n",
       "      <td>DAPHNE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          FILENAME   IDENTITY\n",
       "0  TRAIN_00001.jpg  BALTHAZAR\n",
       "1  TRAIN_00002.jpg      SIMON\n",
       "2  TRAIN_00003.jpg      BENES\n",
       "3  TRAIN_00004.jpg    LA LOVE\n",
       "4  TRAIN_00005.jpg     DAPHNE"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# See how the data looks like\n",
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e3fe997a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique classes (unique characters):  44\n"
     ]
    }
   ],
   "source": [
    "# Get the train labels\n",
    "train_labels = [str(word) for word in train_csv[\"IDENTITY\"].to_numpy()]\n",
    "\n",
    "# #Extract all the unique characters\n",
    "unique_characters = set(char for word in train_labels for char in word)\n",
    "\n",
    "# Define the number of classes (for labels) based on the number of unique characters\n",
    "n_classes = len(unique_characters)\n",
    "\n",
    "print(\"Number of unique classes (unique characters): \", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0bcbc194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the longest label in the dataset:  24\n"
     ]
    }
   ],
   "source": [
    "# Get the maximum length that a label can have\n",
    "MAX_LABEL_LENGTH = max(map(len, train_labels))\n",
    "\n",
    "print(\"The length of the longest label in the dataset: \", MAX_LABEL_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ae960a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv['FILENAME'] = [train_image_dir + f\"/{filename}\" for filename in train_csv['FILENAME']]\n",
    "test_csv['FILENAME'] = [test_image_dir + f\"/{filename}\" for filename in test_csv['FILENAME']]\n",
    "valid_csv['FILENAME'] = [valid_image_dir + f\"/{filename}\" for filename in valid_csv['FILENAME']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "716b7e4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FILENAME</th>\n",
       "      <th>IDENTITY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>D:\\Projects\\Handwritten OCR\\Dataset\\train_v2\\t...</td>\n",
       "      <td>BALTHAZAR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>D:\\Projects\\Handwritten OCR\\Dataset\\train_v2\\t...</td>\n",
       "      <td>SIMON</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>D:\\Projects\\Handwritten OCR\\Dataset\\train_v2\\t...</td>\n",
       "      <td>BENES</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>D:\\Projects\\Handwritten OCR\\Dataset\\train_v2\\t...</td>\n",
       "      <td>LA LOVE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>D:\\Projects\\Handwritten OCR\\Dataset\\train_v2\\t...</td>\n",
       "      <td>DAPHNE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            FILENAME   IDENTITY\n",
       "0  D:\\Projects\\Handwritten OCR\\Dataset\\train_v2\\t...  BALTHAZAR\n",
       "1  D:\\Projects\\Handwritten OCR\\Dataset\\train_v2\\t...      SIMON\n",
       "2  D:\\Projects\\Handwritten OCR\\Dataset\\train_v2\\t...      BENES\n",
       "3  D:\\Projects\\Handwritten OCR\\Dataset\\train_v2\\t...    LA LOVE\n",
       "4  D:\\Projects\\Handwritten OCR\\Dataset\\train_v2\\t...     DAPHNE"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "813436e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Char to num\n",
    "char_to_num = StringLookup(vocabulary = list(unique_characters), mask_token=None)\n",
    "num_to_char = StringLookup(vocabulary = char_to_num.get_vocabulary(), mask_token=None, invert=True)\n",
    "\n",
    "len(char_to_num.get_vocabulary()) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5e950b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function gets the image path and reads the image using TensorFlow,\n",
    "then the image will be decoded and will be converted to float datatype.\n",
    "Next resize and transpose will be applied to it.\n",
    "In the final step the image will be converted to a Numpy Array using tf.cast\n",
    "'''\n",
    "\n",
    "\n",
    "def load_image(image_path):    \n",
    "    # Read the image\n",
    "    image = tf.io.read_file(image_path)\n",
    "\n",
    "    # Decode the image into numerical format uint8\n",
    "    decoded_image = tf.image.decode_jpeg(contents=image, channels=1)\n",
    "\n",
    "    # Convert image datatype to float32\n",
    "    convert_image = tf.image.convert_image_dtype(image=decoded_image, dtype=tf.float32)\n",
    "\n",
    "    # Resize and transpose\n",
    "    resized_image = tf.image.resize(images=convert_image, size=IMG_SIZE)\n",
    "    image = tf.transpose(resized_image, perm=[1, 0, 2])\n",
    "\n",
    "    # Convert to numpy array (Tensor)\n",
    "    image_array = tf.cast(image, dtype=tf.float32)\n",
    "\n",
    "    return image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c520540e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This function takes an image path and label as input and returns the dictionary containing the processed image tensor and the label tensor.\n",
    "First, it loads the image using the load_image function, that we just made above.\n",
    "Then it converts the given label string into a sequence of Unicode characters using the unicode_split function.\n",
    "Next, it uses the char_to_num layer to convert each character in the label to a numerical reperesentation. It pads the numerical representation with a special class (n_classes) to ensure that all the labels have the same length (MAX_LABEL_LENGTH).\n",
    "Finally, it returns a dictionary containing the processed image tensor and the label tensor.\n",
    "'''\n",
    "\n",
    "\n",
    "def encode_single_sample(image_path, label:str):\n",
    "   # Get the image\n",
    "   image = load_image(image_path)\n",
    "\n",
    "   # Convert the label intop characters\n",
    "   chars = tf.strings.unicode_split(label, input_encoding='UTF-8')\n",
    "\n",
    "   # Convert the characters into vectors\n",
    "   vectors = char_to_num(chars)\n",
    "\n",
    "   # Pad label\n",
    "   pad_size = MAX_LABEL_LENGTH - tf.shape(vectors)[0]\n",
    "   vectors = tf.pad(vectors, paddings = [[0, pad_size]], constant_values = n_classes+1)\n",
    "\n",
    "   return {'image': image, 'label': vectors}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "09615397",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Dataset\n",
    "train_ds = tfd.Dataset.from_tensor_slices(\n",
    "    (np.array(train_csv['FILENAME'].to_list()), np.array(train_csv['IDENTITY'].to_list()))\n",
    ").shuffle(1000).map(encode_single_sample, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "# Validation Dataset\n",
    "valid_ds = tfd.Dataset.from_tensor_slices(\n",
    "    (np.array(valid_csv['FILENAME'].to_list()), np.array(valid_csv['IDENTITY'].to_list()))\n",
    ").shuffle(1000).map(encode_single_sample, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "\n",
    "# Test Dataset\n",
    "test_ds = tfd.Dataset.from_tensor_slices(\n",
    "    (np.array(test_csv['FILENAME'].to_list()), np.array(test_csv['IDENTITY'].to_list()))\n",
    ").shuffle(1000).map(encode_single_sample, num_parallel_calls=AUTOTUNE).batch(BATCH_SIZE).prefetch(AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4dc7a775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Size   :  24000\n",
      "Validation Data Size :  16000\n",
      "Test Data Size       :  9600\n"
     ]
    }
   ],
   "source": [
    "# Check the data redistribution\n",
    "print(\"Training Data Size   : \", tfd.Dataset.cardinality(train_ds).numpy() * BATCH_SIZE)\n",
    "print(\"Validation Data Size : \", tfd.Dataset.cardinality(valid_ds).numpy() * BATCH_SIZE)\n",
    "print(\"Test Data Size       : \", tfd.Dataset.cardinality(test_ds).numpy() * BATCH_SIZE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
